{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/StableAnimator-jupyter/blob/main/StableAnimator_jupyter.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjYy0F2gZIPR"
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 torchtext==0.18.0 torchdata==0.8.0 --extra-index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install xformers==0.0.28.post3\n",
    "!apt install aria2 -qqy\n",
    "\n",
    "%cd /content\n",
    "!git clone https://github.com/Francis-Rings/StableAnimator\n",
    "%cd /content/StableAnimator\n",
    "\n",
    "!pip install diffusers transformers accelerate timm decord einops scipy pandas coloredlogs flatbuffers numpy \n",
    "!pip install packaging protobuf sympy imageio-ffmpeg insightface facexlib opencv-python-headless gradio onnxruntime-gpu\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/Animation/face_encoder.pth -d /content/StableAnimator/checkpoints/Animation -o face_encoder.pth\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/Animation/pose_net.pth -d /content/StableAnimator/checkpoints/Animation -o pose_net.pth\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/Animation/unet.pth -d /content/StableAnimator/checkpoints/Animation -o unet.pth\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/DWPose/dw-ll_ucoco_384.onnx -d /content/StableAnimator/checkpoints/DWPose -o dw-ll_ucoco_384.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/DWPose/yolox_l.onnx -d /content/StableAnimator/checkpoints/DWPose -o yolox_l.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/models/antelopev2/1k3d68.onnx -d /content/StableAnimator/checkpoints/models/antelopev2 -o 1k3d68.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/models/antelopev2/2d106det.onnx -d /content/StableAnimator/checkpoints/models/antelopev2 -o 2d106det.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/models/antelopev2/genderage.onnx -d /content/StableAnimator/checkpoints/models/antelopev2 -o genderage.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/models/antelopev2/glintr100.onnx -d /content/StableAnimator/checkpoints/models/antelopev2 -o glintr100.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/models/antelopev2/scrfd_10g_bnkps.onnx -d /content/StableAnimator/checkpoints/models/antelopev2 -o scrfd_10g_bnkps.onnx\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/raw/main/stable-video-diffusion-img2vid-xt/model_index.json -d /content/StableAnimator/checkpoints/stable-video-diffusion-img2vid-xt -o model_index.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/raw/main/stable-video-diffusion-img2vid-xt/feature_extractor/preprocessor_config.json -d /content/StableAnimator/checkpoints/stable-video-diffusion-img2vid-xt/feature_extractor -o preprocessor_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/raw/main/stable-video-diffusion-img2vid-xt/image_encoder/config.json -d /content/StableAnimator/checkpoints/stable-video-diffusion-img2vid-xt/image_encoder -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/stable-video-diffusion-img2vid-xt/image_encoder/model.safetensors -d /content/StableAnimator/checkpoints/stable-video-diffusion-img2vid-xt/image_encoder -o model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/raw/main/stable-video-diffusion-img2vid-xt/scheduler/scheduler_config.json -d /content/StableAnimator/checkpoints/stable-video-diffusion-img2vid-xt/scheduler -o scheduler_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/raw/main/stable-video-diffusion-img2vid-xt/unet/config.json -d /content/StableAnimator/checkpoints/stable-video-diffusion-img2vid-xt/unet -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/stable-video-diffusion-img2vid-xt/unet/diffusion_pytorch_model.safetensors -d /content/StableAnimator/checkpoints/stable-video-diffusion-img2vid-xt/unet -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/raw/main/stable-video-diffusion-img2vid-xt/vae/config.json -d /content/StableAnimator/checkpoints/stable-video-diffusion-img2vid-xt/vae -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/stable-video-diffusion-img2vid-xt/vae/diffusion_pytorch_model.safetensors -d /content/StableAnimator/checkpoints/stable-video-diffusion-img2vid-xt/vae -o diffusion_pytorch_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/stable-video-diffusion-img2vid-xt/stable-video-diffusion-img2vid-xt_xt.safetensors -d /content/StableAnimator/checkpoints/stable-video-diffusion-img2vid-xt -o stable-video-diffusion-img2vid-xt_xt.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/stable-video-diffusion-img2vid-xt/stable-video-diffusion-img2vid-xt_xt_image_decoder.safetensors -d /content/StableAnimator/checkpoints/stable-video-diffusion-img2vid-xt -o stable-video-diffusion-img2vid-xt_xt_image_decoder.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/FrancisRing/StableAnimator/resolve/main/inference.zip -d /content/StableAnimator/checkpoints -o inference.zip\n",
    "!unzip /content/StableAnimator/checkpoints/inference.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from diffusers.models.attention_processor import XFormersAttnProcessor\n",
    "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "import torch\n",
    "from diffusers import AutoencoderKLTemporalDecoder, EulerDiscreteScheduler\n",
    "\n",
    "from animation.modules.attention_processor import AnimationAttnProcessor\n",
    "from animation.modules.attention_processor_normalized import AnimationIDAttnNormalizedProcessor\n",
    "from animation.modules.face_model import FaceModel\n",
    "from animation.modules.id_encoder import FusionFaceId\n",
    "from animation.modules.pose_net import PoseNet\n",
    "from animation.modules.unet import UNetSpatioTemporalConditionModel\n",
    "from animation.pipelines.inference_pipeline_animation import InferenceAnimationPipeline\n",
    "import random\n",
    "\n",
    "import gradio as gr\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "pretrained_model_name_or_path = \"checkpoints/stable-video-diffusion-img2vid-xt\"\n",
    "revision = None\n",
    "posenet_model_name_or_path = \"checkpoints/Animation/pose_net.pth\"\n",
    "face_encoder_model_name_or_path = \"checkpoints/Animation/face_encoder.pth\"\n",
    "unet_model_name_or_path = \"checkpoints/Animation/unet.pth\"\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder, width, height):\n",
    "    images = []\n",
    "    files = os.listdir(folder)\n",
    "    png_files = [f for f in files if f.endswith('.png')]\n",
    "    png_files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "    for filename in png_files:\n",
    "        img = Image.open(os.path.join(folder, filename)).convert('RGB')\n",
    "        img = img.resize((width, height))\n",
    "        images.append(img)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def save_frames_as_png(frames, output_path):\n",
    "    pil_frames = [Image.fromarray(frame) if isinstance(frame, np.ndarray) else frame for frame in frames]\n",
    "    num_frames = len(pil_frames)\n",
    "    for i in range(num_frames):\n",
    "        pil_frame = pil_frames[i]\n",
    "        save_path = os.path.join(output_path, f'frame_{i}.png')\n",
    "        pil_frame.save(save_path)\n",
    "\n",
    "\n",
    "def save_frames_as_mp4(frames, output_mp4_path, fps):\n",
    "    print(\"Starting saving the frames as mp4\")\n",
    "    height, width, _ = frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 'H264' for better quality\n",
    "    out = cv2.VideoWriter(output_mp4_path, fourcc, fps, (width, height))\n",
    "    for frame in frames:\n",
    "       frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "       out.write(frame_bgr)\n",
    "    out.release()\n",
    "\n",
    "\n",
    "def export_to_gif(frames, output_gif_path, fps):\n",
    "    \"\"\"\n",
    "    Export a list of frames to a GIF.\n",
    "\n",
    "    Args:\n",
    "    - frames (list): List of frames (as numpy arrays or PIL Image objects).\n",
    "    - output_gif_path (str): Path to save the output GIF.\n",
    "    - duration_ms (int): Duration of each frame in milliseconds.\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert numpy arrays to PIL Images if needed\n",
    "    pil_frames = [Image.fromarray(frame) if isinstance(\n",
    "        frame, np.ndarray) else frame for frame in frames]\n",
    "\n",
    "    pil_frames[0].save(output_gif_path.replace('.mp4', '.gif'),\n",
    "                       format='GIF',\n",
    "                       append_images=pil_frames[1:],\n",
    "                       save_all=True,\n",
    "                       duration=125,\n",
    "                       loop=0)\n",
    "\n",
    "\n",
    "def generate(\n",
    "    image_input: str, \n",
    "    pose_input: str, \n",
    "    width: int, \n",
    "    height: int, \n",
    "    guidance_scale: float, \n",
    "    num_inference_steps: int, \n",
    "    fps: int, \n",
    "    frames_overlap: int, \n",
    "    tile_size: int, \n",
    "    noise_aug_strength: float, \n",
    "    decode_chunk_size: int,\n",
    "    seed: int,\n",
    "):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = Path(\"outputs\")\n",
    "    output_dir = os.path.join(output_dir, timestamp)\n",
    "    if seed == -1:\n",
    "        seed = random.randint(1, 2**20 - 1)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "    pipeline = InferenceAnimationPipeline(\n",
    "        vae=vae,\n",
    "        image_encoder=image_encoder,\n",
    "        unet=unet,\n",
    "        scheduler=noise_scheduler,\n",
    "        feature_extractor=feature_extractor,\n",
    "        pose_net=pose_net,\n",
    "        face_encoder=face_encoder,\n",
    "    ).to(device=device, dtype=dtype)\n",
    "\n",
    "    validation_image_path = image_input\n",
    "    validation_image = Image.open(image_input).convert('RGB')\n",
    "    validation_control_images = load_images_from_folder(pose_input, width=width, height=height)\n",
    "\n",
    "    num_frames = len(validation_control_images)\n",
    "    face_model.face_helper.clean_all()\n",
    "    validation_face = cv2.imread(validation_image_path)\n",
    "    validation_image_bgr = cv2.cvtColor(validation_face, cv2.COLOR_RGB2BGR)\n",
    "    validation_image_face_info = face_model.app.get(validation_image_bgr)\n",
    "    if len(validation_image_face_info) > 0:\n",
    "        validation_image_face_info = sorted(validation_image_face_info, key=lambda x: (x['bbox'][2] - x['bbox'][0]) * (x['bbox'][3] - x['bbox'][1]))[-1]\n",
    "        validation_image_id_ante_embedding = validation_image_face_info['embedding']\n",
    "    else:\n",
    "        validation_image_id_ante_embedding = None\n",
    "\n",
    "    if validation_image_id_ante_embedding is None:\n",
    "        face_model.face_helper.read_image(validation_image_bgr)\n",
    "        face_model.face_helper.get_face_landmarks_5(only_center_face=True)\n",
    "        face_model.face_helper.align_warp_face()\n",
    "\n",
    "        if len(face_model.face_helper.cropped_faces) == 0:\n",
    "            validation_image_id_ante_embedding = np.zeros((512,))\n",
    "        else:\n",
    "            validation_image_align_face = face_model.face_helper.cropped_faces[0]\n",
    "            print('fail to detect face using insightface, extract embedding on align face')\n",
    "            validation_image_id_ante_embedding = face_model.handler_ante.get_feat(validation_image_align_face)\n",
    "\n",
    "    # generator = torch.Generator(device=accelerator.device).manual_seed(23123134)\n",
    "\n",
    "    decode_chunk_size = decode_chunk_size\n",
    "    video_frames = pipeline(\n",
    "        image=validation_image,\n",
    "        image_pose=validation_control_images,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        num_frames=num_frames,\n",
    "        tile_size=tile_size,\n",
    "        tile_overlap=frames_overlap,\n",
    "        decode_chunk_size=decode_chunk_size,\n",
    "        motion_bucket_id=127.,\n",
    "        fps=7,\n",
    "        min_guidance_scale=guidance_scale,\n",
    "        max_guidance_scale=guidance_scale,\n",
    "        noise_aug_strength=noise_aug_strength,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        generator=generator,\n",
    "        output_type=\"pil\",\n",
    "        validation_image_id_ante_embedding=validation_image_id_ante_embedding,\n",
    "    ).frames[0]\n",
    "\n",
    "    out_file = os.path.join(\n",
    "        output_dir,\n",
    "        f\"animation_video.mp4\",\n",
    "    )\n",
    "    for i in range(num_frames):\n",
    "        img = video_frames[i]\n",
    "        video_frames[i] = np.array(img)\n",
    "\n",
    "    png_out_file = os.path.join(output_dir, \"animated_images\")\n",
    "    os.makedirs(png_out_file, exist_ok=True)\n",
    "\n",
    "    save_frames_as_mp4(video_frames, out_file, fps)\n",
    "    export_to_gif(video_frames, out_file, fps)\n",
    "    save_frames_as_png(video_frames, png_out_file)\n",
    "\n",
    "    seed_update = gr.update(visible=True, value=seed)\n",
    "    \n",
    "    return out_file, seed_update\n",
    "\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft(), analytics_enabled=False) as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "            <div>\n",
    "                <h2 style=\"font-size: 30px;text-align: center;\">StableAnimator</h2>\n",
    "            </div>\n",
    "            <div style=\"text-align: center;\">\n",
    "                <a href=\"https://github.com/Francis-Rings/StableAnimator\">üåê Github</a> |\n",
    "                <a href=\"https://arxiv.org/abs/2411.17697\">üìú arXiv </a>\n",
    "            </div>\n",
    "            <div style=\"text-align: center; font-weight: bold; color: red;\">\n",
    "                ‚ö†Ô∏è This demo is for academic research and experiential use only.\n",
    "            </div>\n",
    "            \"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            with gr.Group():\n",
    "                image_input = gr.Image(label=\"Reference Image\", type=\"filepath\")\n",
    "                pose_input = gr.Textbox(label=\"Driven Poses\", placeholder=\"Please enter your driven pose directory here.\")\n",
    "            with gr.Group():\n",
    "                with gr.Row():\n",
    "                    width = gr.Number(label=\"Width (supports only 512√ó512 and 576√ó1024)\", value=512)\n",
    "                    height = gr.Number(label=\"Height (supports only 512√ó512 and 576√ó1024)\", value=512)\n",
    "                with gr.Row():\n",
    "                    guidance_scale = gr.Number(label=\"Guidance scale (recommended 3.0)\", value=3.0, step=0.1, precision=1)\n",
    "                    num_inference_steps = gr.Number(label=\"Inference steps (recommended 25)\", value=20)\n",
    "                with gr.Row():\n",
    "                    fps = gr.Number(label=\"FPS\", value=8)\n",
    "                    frames_overlap = gr.Number(label=\"Overlap Frames (recommended 4)\", value=4)\n",
    "                with gr.Row():\n",
    "                    tile_size = gr.Number(label=\"Tile Size (recommended 16)\", value=16)\n",
    "                    noise_aug_strength = gr.Number(label=\"Noise Augmentation Strength (recommended 0.02)\", value=0.02, step=0.01, precision=2)\n",
    "                with gr.Row():\n",
    "                    decode_chunk_size = gr.Number(label=\"Decode Chunk Size (recommended 4 or 16)\", value=4)\n",
    "                    seed = gr.Number(label=\"Random Seed (Enter a positive number, -1 for random)\", value=-1)\n",
    "            generate_button = gr.Button(\"üé¨ Generate The Video\")\n",
    "        with gr.Column():\n",
    "            video_output = gr.Video(label=\"Generate The Video\")\n",
    "            with gr.Row():\n",
    "                seed_text = gr.Number(label=\"Video Generation Seed\", visible=False, interactive=False)\n",
    "    gr.Examples([\n",
    "        [\"inference/case-1/reference.png\",\"inference/case-1/poses\",512,512],\n",
    "        [\"inference/case-2/reference.png\",\"inference/case-2/poses\",512,512],\n",
    "        [\"inference/case-3/reference.png\",\"inference/case-3/poses\",512,512],\n",
    "        [\"inference/case-4/reference.png\",\"inference/case-4/poses\",512,512],\n",
    "        [\"inference/case-5/reference.png\",\"inference/case-5/poses\",576,1024],\n",
    "    ], inputs=[image_input, pose_input, width, height])\n",
    "\n",
    "\n",
    "    generate_button.click(\n",
    "        generate,\n",
    "        inputs=[image_input, pose_input, width, height, guidance_scale, num_inference_steps, fps, frames_overlap, tile_size, noise_aug_strength, decode_chunk_size, seed],\n",
    "        outputs=[video_output, seed_text],\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    feature_extractor = CLIPImageProcessor.from_pretrained(pretrained_model_name_or_path, subfolder=\"feature_extractor\", revision=revision)\n",
    "    noise_scheduler = EulerDiscreteScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(pretrained_model_name_or_path, subfolder=\"image_encoder\", revision=revision)\n",
    "    vae = AutoencoderKLTemporalDecoder.from_pretrained(pretrained_model_name_or_path, subfolder=\"vae\", revision=revision)\n",
    "    unet = UNetSpatioTemporalConditionModel.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"unet\",\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "    pose_net = PoseNet(noise_latent_channels=unet.config.block_out_channels[0])\n",
    "    face_encoder = FusionFaceId(\n",
    "        cross_attention_dim=1024,\n",
    "        id_embeddings_dim=512,\n",
    "        # clip_embeddings_dim=image_encoder.config.hidden_size,\n",
    "        clip_embeddings_dim=1024,\n",
    "        num_tokens=4, )\n",
    "    face_model = FaceModel()\n",
    "\n",
    "    lora_rank = 128\n",
    "    attn_procs = {}\n",
    "    unet_svd = unet.state_dict()\n",
    "\n",
    "    for name in unet.attn_processors.keys():\n",
    "        if \"transformer_blocks\" in name and \"temporal_transformer_blocks\" not in name:\n",
    "            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "            if name.startswith(\"mid_block\"):\n",
    "                hidden_size = unet.config.block_out_channels[-1]\n",
    "            elif name.startswith(\"up_blocks\"):\n",
    "                block_id = int(name[len(\"up_blocks.\")])\n",
    "                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "            elif name.startswith(\"down_blocks\"):\n",
    "                block_id = int(name[len(\"down_blocks.\")])\n",
    "                hidden_size = unet.config.block_out_channels[block_id]\n",
    "            if cross_attention_dim is None:\n",
    "                # print(f\"This is AnimationAttnProcessor: {name}\")\n",
    "                attn_procs[name] = AnimationAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, rank=lora_rank)\n",
    "            else:\n",
    "                # print(f\"This is AnimationIDAttnProcessor: {name}\")\n",
    "                layer_name = name.split(\".processor\")[0]\n",
    "                weights = {\n",
    "                    \"to_k_ip.weight\": unet_svd[layer_name + \".to_k.weight\"],\n",
    "                    \"to_v_ip.weight\": unet_svd[layer_name + \".to_v.weight\"],\n",
    "                }\n",
    "                attn_procs[name] = AnimationIDAttnNormalizedProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, rank=lora_rank)\n",
    "                attn_procs[name].load_state_dict(weights, strict=False)\n",
    "        elif \"temporal_transformer_blocks\" in name:\n",
    "            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "            if name.startswith(\"mid_block\"):\n",
    "                hidden_size = unet.config.block_out_channels[-1]\n",
    "            elif name.startswith(\"up_blocks\"):\n",
    "                block_id = int(name[len(\"up_blocks.\")])\n",
    "                hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "            elif name.startswith(\"down_blocks\"):\n",
    "                block_id = int(name[len(\"down_blocks.\")])\n",
    "                hidden_size = unet.config.block_out_channels[block_id]\n",
    "            if cross_attention_dim is None:\n",
    "                attn_procs[name] = XFormersAttnProcessor()\n",
    "            else:\n",
    "                attn_procs[name] = XFormersAttnProcessor()\n",
    "    unet.set_attn_processor(attn_procs)\n",
    "\n",
    "    # resume the previous checkpoint\n",
    "    if posenet_model_name_or_path is not None and face_encoder_model_name_or_path is not None and unet_model_name_or_path is not None:\n",
    "        print(\"Loading existing posenet weights, face_encoder weights and unet weights.\")\n",
    "        if posenet_model_name_or_path.endswith(\".pth\"):\n",
    "            pose_net_state_dict = torch.load(posenet_model_name_or_path, map_location=\"cpu\")\n",
    "            pose_net.load_state_dict(pose_net_state_dict, strict=True)\n",
    "        else:\n",
    "            print(\"posenet weights loading fail\")\n",
    "            print(1/0)\n",
    "        if face_encoder_model_name_or_path.endswith(\".pth\"):\n",
    "            face_encoder_state_dict = torch.load(face_encoder_model_name_or_path, map_location=\"cpu\")\n",
    "            face_encoder.load_state_dict(face_encoder_state_dict, strict=True)\n",
    "        else:\n",
    "            print(\"face_encoder weights loading fail\")\n",
    "            print(1/0)\n",
    "        if unet_model_name_or_path.endswith(\".pth\"):\n",
    "            unet_state_dict = torch.load(unet_model_name_or_path, map_location=\"cpu\")\n",
    "            unet.load_state_dict(unet_state_dict, strict=True)\n",
    "        else:\n",
    "            print(\"unet weights loading fail\")\n",
    "            print(1/0)\n",
    "\n",
    "    vae.requires_grad_(False)\n",
    "    image_encoder.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "    pose_net.requires_grad_(False)\n",
    "    face_encoder.requires_grad_(False)\n",
    "\n",
    "    total_vram_in_gb = torch.cuda.get_device_properties(0).total_memory / 1073741824\n",
    "    print(f'\\033[32mCUDA versionÔºö{torch.version.cuda}\\033[0m')\n",
    "    print(f'\\033[32mPytorch versionÔºö{torch.__version__}\\033[0m')\n",
    "    print(f'\\033[32mGPU TypeÔºö{torch.cuda.get_device_name()}\\033[0m')\n",
    "    print(f'\\033[32mGPU MemoryÔºö{total_vram_in_gb:.2f}GB\\033[0m')\n",
    "    if torch.cuda.get_device_capability()[0] >= 8:\n",
    "        print(f'\\033[32mSupports BF16, use BF16\\033[0m')\n",
    "        dtype = torch.bfloat16\n",
    "    else:\n",
    "        print(f'\\033[32mBF16 is not supported, use FP16. The 5B model is not recommended\\033[0m')\n",
    "        dtype = torch.float16\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    demo.queue().launch(inline=False, share=True, debug=True, server_name='0.0.0.0', server_port=7860, allowed_paths=[\"/content\"])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
